read=progress
title=Kafka: The Definitive Guide
summary="Kafka: The Definitive Guide" is a must-have book for anyone working with the distributed event streaming platform Apache Kafka.
authors=Gwen Shapira, Todd Palino, Rajini Sivaram & Krit Petty
publisher=O'REILLY
published=December 2021
date=2024-01-24
type=booknote
tags=kafka
status=published
amazonLink=https://a.co/d/7GZmuD8
orreillyLink=https://www.oreilly.com/library/view/kafka-the-definitive/9781492043072/

ogImg=notes/2024/images/kafka-the-definitive-guide_social.png
imageSrc=notes/2024/images/kafka-the-definitive-guide_social.png
imageAlt=Image of the book cover: Kafka: The Definitive Guide

ogAuthor=Asiel Leal Celdeiro
authorHandle=lealceldeiro
authorProfileImage=/img/author/lealceldeiro.png
~~~~~~
<p>
    Main notes taken from the book
    <a href="https://a.co/d/7GZmuD8" target="_blank">
        Kafka: The Definitive Guide: Real-Time Data and Stream Processing at Scale
    </a>.
</p>

<h2>Chapter 1: Meet Kafka</h2>
<p>
    Data within Kafka is stored durably, in order, and can be read deterministically.
</p>
<p>
    In addition, the data can be distributed within the system to provide additional protections against failures, as
    well as significant opportunities for scaling performance.
</p>
<p>
    The unit of data within Kafka is the <em>message</em>.
</p>
<p>
    A message can have an optional piece of metadata, which is referred to as a <em>key</em>.
</p>
<p>
    The message and the key are byte arrays and have no specific meaning to Kafka.
</p>
<p>
    The <em>offset</em>, an integer value that continually increases, is another piece of metadata that Kafka adds to
    each message as it is produced.
</p>
<p>
    A single Kafka server is called a <em>broker</em>.
</p>
<p>
    Kafka brokers are designed to operate as part of a <em>cluster</em>.
</p>
<p>
    Within a cluster of brokers, one broker will also function as the cluster <em>controller</em>.
</p>
<p>
    A partition is owned by a single broker in the cluster, and that broker is called the <em>leader</em> of the
    partition.
</p>
<p>
    A replicated partition is assigned to additional brokers, called <em>followers</em> of the partition.
</p>

<h3>Some pros</h3>

<ul>
    <li>Support for multiple producers</li>
    <li>Support for multiple consumers</li>
    <li>Disk-based retention</li>
    <li>Scalable</li>
    <li>High performance</li>
</ul>

<h3>Use Cases</h3>

<ul>
    <li>Activity tracking</li>
    <li>Messaging</li>
    <li>Metrics and logging</li>
    <li>Commit log</li>
    <li>Stream processing</li>
</ul>

<h2>Chapter 2: Installing Kafka</h2>

<h3>My own experience running Kafka locally with Docker</h3>

<p>
    For this to work, <a href="https://www.docker.com/" target="_blank">Docker</a> must be installed.
</p>
<p>
    The book starts by mentioning the installation of Kafka and ZooKeeper, but after
    <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-833%3A+Mark+KRaft+as+Production+Ready"
       target="_blank">
        KRaft was marked as Production Ready
    </a>, only the Kafka image is actually needed when the version 2.8 or later of Apache Kafka is used.
</p>
<p>
    The following example uses
    <a href="https://hub.docker.com/r/bitnami/kafka" target="_blank">
        bitnami/kafka
    </a> and is based in that official documentation examples.
</p>
<ul>
    <li>
        <strong>Create a Docker network</strong> (see <a href="https://docs.docker.com/network/" target="_blank">networking</a>)
        <blockquote>
            This is to get the Apache Kafka server running inside the container accessed by another Apache Kafka client,
            running inside another container. This is possible because containers attached to the same network can
            communicate with each other using the container name as the hostname.
        </blockquote>

        <pre><code class="language-shell">
            docker network create app-tier --driver bridge
        </code></pre>
    </li>
    <li>
        <strong>Launch the Apache Kafka server</strong> instance from a docker container
        <blockquote>
            Use the <code>--network app-tier</code> argument to run the docker command to attach the Apache Kafka
            container to the <code>app-tier</code> network.
        </blockquote>
        <pre><code class="language-shell">
            docker run -d -p 9094:9094 --name kafka-server --hostname kafka-server --network app-tier \
                       -e KAFKA_CFG_NODE_ID=0 \
                       -e KAFKA_CFG_PROCESS_ROLES=controller,broker \
                       -e KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093,EXTERNAL://:9094 \
                       -e KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://:9092,EXTERNAL://localhost:9094 \
                       -e KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,EXTERNAL:PLAINTEXT,PLAINTEXT:PLAINTEXT \
                       -e KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka-server:9093 \
                       -e KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER \
                       bitnami/kafka:3.6.1
        </code></pre>

        To create a cluster (with more than one broker) see
        <a href="https://github.com/bitnami/containers/blob/main/bitnami/kafka/README.md#setting-up-a-apache-kafka-cluster"
           target="_blank">
            Setting up an Apache Kafka cluster
        </a>.
        <blockquote>
            The following parameters are optional in this minimal setup, and are only required if the server will be
            accessed from the same local machine by another app running from the host machine.
            <ul>
                <li><code>-p 9094:9094</code></li>
                <li><code>EXTERNAL://:9094</code> in <code>KAFKA_CFG_LISTENERS</code></li>
                <li><code>-e KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://:9092,EXTERNAL://localhost:9094 \</code></li>
                <li>and, <code>EXTERNAL:PLAINTEXT</code> in <code>KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP</code></li>
            </ul>
            Also, after the container is (created) and ran the first time, then, in subsequent runs, only this is
            required to start the container:
            <pre><code class="language-shell">
                docker container start kafka-server
            </code></pre>
        </blockquote>
    </li>
    <li>
        <strong>Launch an Apache Kafka client</strong> instance from another container and list topics
        <blockquote>
            This new container instance running the client will connect to the server created in the previous step.
        </blockquote>
        <pre><code class="language-shell">
        docker run -it --rm --network app-tier bitnami/kafka:3.6.1 kafka-topics.sh --bootstrap-server kafka-server:9092 --list
        </code></pre>
        <blockquote>At this point there shouldn't be any topics displayed.</blockquote>
    </li>
    <li>
        <strong>Create a topic</strong>
        <pre><code class="language-shell">
            docker run -it --rm --network app-tier bitnami/kafka:3.6.1 kafka-topics.sh --bootstrap-server kafka-server:9092 \
                       --create --replication-factor 1 --partitions 1 --topic topic1
        </code></pre>
    </li>
    <li>
        <strong>Verify the previously created topic</strong>
        <pre><code class="language-shell">
            docker run -it --rm --network app-tier bitnami/kafka:3.6.1 kafka-topics.sh --bootstrap-server kafka-server:9092 \
                       --describe --topic topic1
        </code></pre>
    </li>
    <li>
        <strong>Produce messages</strong> to the previously created topic
        <pre><code class="language-shell">
            docker run -it --rm --network app-tier \
                       bitnami/kafka:3.6.1 kafka-console-producer.sh --bootstrap-server kafka-server:9092 --topic topic1
        </code></pre>
        <blockquote>
            When the console waits for input (symbol <code>></code> visible) enter some message and hit <em>Enter</em>
            (once for every message). To finish producing messages send an end-of-file (EOF) character to close the
            client. In most common terminals, this is done with <code>Control</code> + <code>D</code> (<code>^D</code>).
            If that doesn't work, then do <code>Ctrl</code> + <code>C</code> (<code>^C</code>).
        </blockquote>
    </li>
    <li>
        <strong>Consume messages</strong> from the previously created topic
        <pre><code class="language-shell">
            docker run -it --rm --network app-tier \
                       bitnami/kafka:3.6.1 kafka-console-consumer.sh --bootstrap-server kafka-server:9092 \
                       --topic topic1 --from-beginning
        </code></pre>
        <details>
            <summary>Docker run info</summary>
            See the
            <a href="https://docs.docker.com/engine/reference/commandline/run/" target="_blank">
                <code>run</code> command
            </a> for more details.

            <ul>
                <li><code>-d</code>: run container in background and print container ID</li>
                <li>
                    <code>-it</code>: instructs Docker to allocate a pseudo-TTY connected to the container's stdin;
                    creating an interactive bash shell in the container.
                </li>
                <li>
                    <code>--rm</code>: automatically remove the container when it exits
                </li>
            </ul>
        </details>
    </li>
</ul>

<h3>General Broker Parameters</h3>

<ul>
    <li><code>broker.id</code>: it must be unique for each broker within a single Kafka cluster</li>
    <li>
        <code>listeners</code>: defined as <code>&lt;protocol&gt;://&lt;hostname&gt;:&lt;port&gt;</code>, i.e.:
        <code>PLAINTEXT://localhost:9092,SSL://:9091</code>
    </li>
    <li>
        <code>zookeeper.connect</code>: location of the ZooKeeper used for storing the broker metadata in the format
        <code>hostname:port/path</code>
    </li>
    <li>
        <code>log.dirs</code>: comma-separated list of paths on disk where all the message log segments are persisted
    </li>
    <li>
        <code>num.recovery.threads.per.data.dir</code>: number of recovery threads per log directory
    </li>
    <li>
        <code>auto.create.topics.enable</code>: whether to create (or not) automatically topics under certain conditions
    </li>
    <li>
        <code>auto.leader.rebalance.enable</code>: this config can be specified to ensure leadership is balanced as much
        as possible
    </li>
    <li>
        <code>delete.topic.enable</code>: this config can be set to prevent arbitrary deletions of topics (false:
        disabling topic deletion)
    </li>
</ul>

<h3>Selecting Hardware</h3>

<h4>Factors that can contribute to overall performance bottlenecks</h4>

<h5>Disk throughput and capacity</h5>

<p>Faster disk writes will equal lower produce latency.</p>
<p>
    Generally, observations show that HDD drives are typically more useful for clusters with very high storage
    needs but aren't accessed as often, while SSDs are better options if there is a very large number of client
    connections.
</p>
<p>
    The total traffic for a cluster can be balanced across the cluster by having multiple partitions per topic,
    which will allow additional brokers to augment the available capacity if the density on a single broker will not
    suffice.
</p>

<h5>Memory</h5>

<p>
    The normal mode of operation for a Kafka consumer is reading from the end of the partitions, where the
    consumer is caught up and lagging behind the producers very little, if at all. In this situation, the messages the
    consumer is reading are optimally stored in the system's page cache, resulting in faster reads than if the broker
    has to reread the messages from disk. Therefore, having more memory available to the system for page cache will
    improve the performance of consumer clients.
</p>
<p>
    Kafka itself does not need much heap memory configured for the JVM. Even a broker that is handling 150000
    messages per second and a data rate of 200 megabits per second can run with a 5GB heap.
</p>
<p>
    It is not recommended to have Kafka running on a system with any other significant application, as it will
    have to share the use of the page cache. This will decrease the consumer performance for Kafka.
</p>

<h5>Networking</h5>

<p>
    The available network throughput will specify the maximum amount of traffic that Kafka can handle. This can
    be a governing factor, combined with disk storage, for cluster sizing. To prevent the network from being a major
    governing factor, it is recommended to run with at least 10 Gb NICs (Network Interface Cards). Older machines with 1
    Gb NICs are easily saturated and aren't recommended.
</p>

<h5>CPU</h5>

<p>
    Processing power is not as important as disk and memory until you begin to scale Kafka very large, but it
    will affect overall performance of the broker to some extent.
</p>

<h3>Configuring Kafka Clusters</h3>

<p>Typically, the size of a cluster will be bound on the following key areas:</p>
<ul>
    <li>Disk capacity</li>
    <li>Replica capacity per broker</li>
    <li>CPU capacity</li>
    <li>Network capacity</li>
</ul>

<p>
    Currently &mdash;2023&mdash;, in a well-configured environment, it is recommended to not have more than 14000
    partition replicas per broker and 1 million replicas per cluster.
</p>
<p>Requirements in the broker configuration to allow multiple Kafka brokers to join a single cluster:</p>
<ol>
    <li>All brokers must have the same configuration for the <code>zookeeper.connect</code> parameter</li>
    <li>All brokers in the cluster must have a unique value for the <code>broker.id</code> parameter</li>
</ol>

<p>
    The current number of dirty pages can be determined by checking:
</p>
<pre><code class="language-shell">
cat /proc/vmstat | egrep "dirty|writeback"
</code></pre>

<p>
    The most common choices for local filesystems are either Ext4 (fourth extended filesystem) or Extents File
    System (XFS), being XFS the preferred option because it outperforms Ext4 for most workloads with minimal tuning
    required.
</p>

<h3>Production Concerns</h3>

<h4>JVM Garbage Collector Options</h4>

<p>
    As of this writing, the Garbage-First garbage collector (G1GC) is the recommended one, but it's worth
    checking the official
    <a href="https://kafka.apache.org/documentation.html#java" target="_blank">
        Kafka documentation
    </a> related to this topic to get the latest updates; and a best practice is to use if (G1GC) for anything for Java
    1.8 and later.
</p>
<p>
    Configuration options for G1GC used to adjust its performance:
</p>
<ul>
    <li><code>MaxGCPauseMillis</code>: preferred pause time for each garbage-collection cycle</li>
    <li>
        <code>InitiatingHeapOccupancyPercent</code>: specifies the percentage of the total heap that may be in use
        before G1GC will start a collection cycle
    </li>
</ul>

<h4>Datacenter Layout</h4>

<p>A datacenter environment that has a concept of fault zones is preferable.</p>
<p>
    It is recommended to use tools that keep your cluster balanced properly to maintain rack awareness, such as
    <a href="https://github.com/linkedin/cruise-control" target="_blank">
        Cruise Control
    </a>.
</p>
<p>
    Overall, the best practice is to have each Kafka broker in a cluster installed in a different rack, or at
    the very least not share single points of failure for infrastructure services such as power and network.
</p>

<h2>Chapter 3: Kafka Producers: Writing Messages to Kafka</h2>

<h3>Source code</h3>

<p>
    The source code of a sample Java Kafka Producer, based on the content from this chapter, can be found
    <a href="https://github.com/lealceldeiro/kafkaproducer" target="_blank">here</a>.
</p>

<h3>Producer Overview</h3>

<p>Messages to Kafka are produced by creating a <code>ProducerRecord</code>, which includes</p>
<ul>
    <li>(<strong>mandatory</strong>) the topic we want to send the record to</li>
    <li>(<strong>mandatory</strong>) and a value</li>
    <li>(<em>optional</em>) a key</li>
    <li>(<em>optional</em>) a partition</li>
    <li>(<em>optional</em>) a timestamp</li>
    <li>(<em>optional</em>) a collection of headers</li>
</ul>

<p>
    When the <code>ProducerRecord</code> is sent, the first thing the producer will do is serialize the key and value
    objects to byte arrays, so they can be sent over the network.
</p>
<p>
    If a partition isn't explicitly specified, the data is sent to a partitioner (which chooses a partition for
    the message, usually, based on the key).
</p>
<p>
    Then, the record is added to a batch of records that will also be sent to the same topic and partition. A
    separate thread is responsible for sending those batches of records to the appropriate Kafka brokers.
</p>

<h3>Constructing a Kafka Producer</h3>

<p>Mandatory properties of a Kafka producer:</p>
<ul>
    <li>
        <code>bootstrap.servers</code>: List of host:port pairs of brokers that the producer will use to establish
        initial connection to the Kafka cluster.
    </li>
    <li>
        <code>key.serializer</code>: Name of a class that will be used to serialize the keys of the records that will be
        produced to Kafka.
    </li>
    <li>
        <code>value.serializer</code>: Name of a class that will be used to serialize the values of the records that
        will be produced to Kafka.
    </li>
</ul>

<p>
    All the configuration options are available in the
    <a href="https://kafka.apache.org/documentation.html#producerconfigs" target="_blank">
        official docs
    </a>.
</p>

<p>Primary methods of sending messages:</p>
<ul>
    <li>
        Fire-and-forget
    </li>
    <li>
        Synchronous send
    </li>
    <li>
        Asynchronous send
    </li>
</ul>

<h3>Configuring Producers</h3>

<p>
    Some of the parameters that have a significant impact on memory use, performance, and reliability of the
    producers:
</p>
<ul>
    <li>
        <code>client.id</code>: logical identifier for the client and the application it is used in
    </li>
    <li>
        <code>acks</code>: controls how many partition replicas must receive the record before the producer can consider
        the write successful (valid values: <code>0</code>, <code>1</code>, <code>all</code>)
    </li>
    <li>
        <code>max.block.ms</code>: controls how long the producer may block when calling <code>send()</code> and when
        explicitly requesting metadata via <code>partitionsFor()</code>
    </li>
    <li>
        <code>delivery.timeout.ms</code>: limits the amount of time spent from the point a record is ready for sending
        (<code>send()</code> returned successfully and the record is placed in a batch) until either the broker responds
        or the client gives up, including time spent on retries
    </li>
    <li>
        <code>request.timeout.ms</code>: controls how long the producer will wait for a reply from the server when
        sending data
    </li>
    <li>
        <code>retries</code>: control how many times the producer will retry sending the message before giving up and
        notifying the client of an issue. <code>retries=0</code> disables retrying
    </li>
    <li>
        <code>retry.backoff.ms</code>: controls the time to wait when retrying (time-lapse between one and the
        subsequents retry) to send a message
    </li>
    <li>
        <code>linger.ms</code>: controls the amount of time to wait for additional messages before sending the current
        batch
    </li>
    <li>
        <code>buffer.memory</code>: sets the amount of memory the producer will use to buffer messages waiting to be
        sent to brokers
    </li>
    <li>
        <code>compression.type</code>: sets the compression algorithms to be used to compress the data before sending it
        to the brokers. Valid values are <code>snappy</code>, <code>gzip</code>, <code>lz4</code>, and <code>zstd</code>
    </li>
    <li>
        <code>batch.size</code>: controls the amount of memory <strong>in bytes</strong> used for each batch when
        batching multiple messages together by the producer before sending them to the broker
    </li>
</ul>

<h3>Serializers</h3>

<h4>Apache Avro</h4>

<p>Apache Avro is a language-neutral data serialization format.</p>

<p>
    Generating Avro classes can be done either using the `avro-tools.jar` or the Avro Maven plugin, both part of
    Apache Avro (<a href="https://avro.apache.org/docs/current/getting-started-java/" target="_blank">see docs</a>).
</p>

<h3>Partitions</h3>

<p>
    When partitioning keys is important, the easiest solution is to create topics with sufficient partitions,
    see
    <a href="https://www.confluent.io/blog/how-choose-number-topics-partitions-kafka-cluster/" target="_blank">
        How to Choose the Number of Topics/Partitions in a Kafka Cluster?
    </a>
</p>

<h3>Headers</h3>

<p>
    Records can also include headers. Record headers give us the ability to add some metadata about the Kafka
    record, without adding any extra information to the key/value pair of the record itself.
</p>

<h3>Interceptors</h3>

<p>
    Interceptors can help us to modify the behavior of our Kafka client application without modifying its code.
</p>
<p>
    Common use cases for producer interceptors include capturing monitoring and tracing information; enhancing
    the message with standard headers, especially for lineage tracking purposes; and redacting sensitive information.
</p>

<h3>Quotas and Throttling</h3>

<p>
    Kafka brokers have the ability to limit the rate at which messages are produced and consumed. This is done
    via the quota mechanism. Kafka has three quota types: produce, consume, and request.
</p>
<p>
    Produce and consume quotas limit the rate at which clients can send and receive data, measured in bytes per
    second. Request quotas limit the percentage of time the broker spends processing client requests.
</p>
<p>
    When a client reaches its quota, the broker will start throttling the client's requests to prevent it from
    exceeding the quota. This means that the broker will delay responses to client requests; in most clients this will
    automatically reduce the request rate (since the number of in-flight requests is limited) and bring the client
    traffic down to a level allowed by the quota. To protect the broker from misbehaved clients sending additional
    requests while being throttled, the broker will also mute the communication channel with the client for the period
    of time needed to achieve compliance with the quota.
</p>

<h2>Chapter 4: Kafka Consumers: Reading Data from Kafka</h2>

<p>
    The source code of a sample Java Kafka Consumer, based on the content from this chapter, can be found
    <a href="https://github.com/lealceldeiro/kafkaconsumer" target="_blank">here</a>.
</p>
<p>
    Applications that need to read data from Kafka use a KafkaConsumer to subscribe to Kafka topics and receive
    messages from these topics.
</p>

<h4>Consumers and Consumer Groups</h4>

<p>
    A consumer group must be created for each application that needs all the messages from one or more topics.
</p>
<p>
    Consumers are added to an existing consumer group to scale the reading and processing of messages from the
    topics, so each additional consumer in a group will only get a subset of the messages.
</p>

<h4>Consumer Groups and Partition Rebalance</h4>

<p>
    Moving partition ownership from one consumer to another is called a <em>rebalance</em>.
</p>
<p>Types of rebalances:</p>
<ul>
    <li>
        Eager rebalances: all consumers stop consuming, give up their ownership of all partitions, rejoin the consumer
        group, and get a brand-new partition assignment.
    </li>
    <li>
        Cooperative rebalances: typically involve reassigning only a small subset of the partitions from one consumer to
        another, and allowing consumers to continue processing records from all the partitions that are not reassigned.
    </li>
</ul>

<p>
    Consumers maintain membership in a consumer group and ownership of the partitions assigned to them by
    sending <em>heartbeats</em> to a Kafka broker designated as the <em>group coordinator</em>.
</p>

<p>The first consumer to join the group becomes the group <em>leader</em>.</p>

<p>
    By default, the identity of a consumer as a member of its consumer group is transient, unless it's
    configured with a unique <code>group.instance.id</code>, which makes the consumer a <em>static</em> member of the
    group.
</p>
<p>
    If two consumers join the same group with the same group.instance.id, the second consumer will get an error
    saying that a consumer with this ID already exists.
</p>

<p>
    To consume records from a Kafka broker: create a <code>KafkaConsumer</code> (create a Java <code>Properties</code>
    instance with the properties to be passed to the consumer; three mandatory properties:
    <code>bootstrap.servers</code>, <code>key.deserializer</code>, and <code>value.deserializer</code>).
</p>
<ul>
    <li>
        <code>bootstrap.servers</code>: connection string to a Kafka cluster
    </li>
    <li>
        <code>key.deserializer</code> and <code>value.deserializer</code>: specify classes that turn Java objects to
        byte arrays
    </li>
</ul>

<p>
    Another common property is <code>group.id</code>, and it specifies the consumer group the <code>KafkaConsumer</code>
    instance belongs to.
</p>

<h4>Creating a Kafka Consumer</h4>

<h5>The Poll Loop</h5>

<p>
    At the heart of the Consumer API is a simple loop for polling the server for more data.
</p>

<h6>Thread Safety</h6>

<p>
    Multiple consumers that belong to the same group cannot coexist in the same thread, and there cannot be
    multiple threads safely use the same consumer. One consumer per thread is the rule.
</p>
<p>
    To run multiple consumers in the same group in one application, each of them needs to run in its own thread.
    It is useful to wrap the consumer logic in its own object and then use Java's <code>ExecutorService</code> to start
    multiple threads, each with its own consumer. See an example in this
    <a href="https://www.confluent.io/blog/tutorial-getting-started-with-the-new-apache-kafka-0-9-consumer-client/"
       target="_blank">
        Tutorial
    </a>.
</p>

<h4>Configuring Consumers</h4>

<p>
    All the consumer configuration is documented in the
    <a href="https://kafka.apache.org/documentation.html#newconsumerconfigs" target="_blank">
        Apache Kafka documentation
    </a>.
</p>
<p>
    Assignment strategies (used to configure `partition.assignment.strategy` with one of
    the <code>org.apache.kafka.clients.consumer.*Assignor</code> values):
</p>
<ul>
    <li>Range</li>
    <li>RoundRobin</li>
    <li>Sticky</li>
    <li>Cooperative Sticky</li>
</ul>

<h4>Standalone Consumer: Why and How to Use a Consumer Without a Group</h4>

<p>
    When it's known exactly which partitions the consumer should read, the consumer can be not subscribed to a
    topic, instead, it is assigned a few partitions. A consumer can either subscribe to topics (and be part of a
    consumer group) or assign itself partitions, but not both at the same time.
</p>

<p>Example:</p>
<pre><code class="language-java">
Duration timeout = Duration.ofMillis(100);
List&lt;PartitionInfo&gt; partitionInfos = null;
partitionInfos = consumer.partitionsFor("topic");

if (partitionInfos != null) {
    for (PartitionInfo partition : partitionInfos) {
        partitions.add(new TopicPartition(partition.topic(), partition.partition()));
    }
    consumer.assign(partitions);

    while (true) {
        ConsumerRecords&lt;String, String&gt; records = consumer.poll(timeout);

        for (ConsumerRecord&lt;String, String&gt; record: records) {
            // do some work with record
        }
        consumer.commitSync();
    }
}
</code></pre>

<h2>Chapter 5: Managing Apache Kafka Programmatically</h2>

<p>
    Kafka's <em>AdminClient</em> is asynchronous. It is useful for application developers who want to create topics
    on the fly and validate that the topics they are using are configured correctly for their application. It is also
    useful for operators and SREs who want to create tooling and automation around Kafka or need to recover from an
    incident.
</p>

<h4>AdminClient Lifecycle: Creating, Configuring, and Closing</h4>

<p>Example:</p>
<pre><code class="language-java">
Properties props = new Properties();
props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
AdminClient admin = AdminClient.create(props);
// Do something with the AdminClient instance
admin.close(Duration.ofSeconds(30));
</code></pre>

<p>
    All <code>AdminClient</code> configuration can be found at the
    <a href="https://kafka.apache.org/documentation/#adminclientconfigs" target="_blank">
        Apache Kafka officialdocumentation
    </a>.
</p>
<p>Some noteworthy ones are:</p>
<ul>
    <li><code>client.dns.lookup</code></li>
    <li><code>request.timeout.ms</code></li>
</ul>
<p>
    <code>AdminClient</code> result objects throw <code>ExecutionException</code> when Kafka responds with an error.
    This is because <code>AdminClient</code> results are wrapped <code>Future</code> objects, and those wrap exceptions.
    The cause of <code>ExecutionException</code> needs to be examined always to get the error that Kafka returned.
</p>

<h2>Chapter 6: Kafka Internals</h2>

<p>
    Kafka uses ZooKeeper's ephemeral node feature to elect a controller and to notify the controller when nodes join and
    leave the cluster. The controller is responsible for electing leaders among the partitions and replicas whenever it
    notices nodes join and leave the cluster. The controller uses the epoch number to prevent a "split brain" scenario
    where two nodes believe each is the current controller.
</p>
<p>
    From the list of replicas for a partition displayed by the <code>kafka-topics.sh</code> tool, the first replica in
    the list is always the preferred leader.
</p>
<p>
    This is true no matter who is the current leader and even if the replicas were reassigned to different brokers using
    the replica reassignment tool.
</p>
<p>When replicas are manually reassigned, the first replica specified will be the preferred one.</p>
<p>Common types of client requests:</p>
<ul>
    <li>Produce requests: Sent by producers and contain messages the clients write to Kafka brokers</li>
    <li>Fetch requests: Sent by consumers and follower replicas when they read messages from Kafka brokers</li>
    <li>Admin requests: Sent by admin clients when performing metadata operations such as creating and deleting topics
    </li>
</ul>

<p>
    New brokers know how to handle old requests, but old brokers don't know how to handle new requests. Now, in release
    <code>0.10.0</code>, <code>ApiVersionRequest</code> was added, which allows clients to ask the broker which versions
    of each request are supported and to use the correct version accordingly. Clients that use this new capability
    correctly will be able to talk to older brokers by using a version of the protocol that is supported by the broker
    they are connecting to.
</p>

<h2>Chapter 7: Reliable Data Delivery</h2>

<p>
    Reliability is not just a matter of specific Kafka features. An entire reliable system needs to be built, including
    the application architecture, the way applications use the producer and consumer APIs, producer and consumer
    configuration, topic configuration, and broker configuration. Making the system more reliable always has trade-offs
    in application complexity, performance, availability, or disk-space usage. By understanding all the options and
    common patterns and understanding requirements for each use case, more informed decisions can be made regarding how
    reliable the application and Kafka deployment need to be and which trade-offs make sense.
</p>

<p>Guarantees provided by Kafka:</p>
<ul>
    <li>
        Kafka provides order guarantee of messages in a partition. If message B was written after message A, using the
        same producer in the same partition, then Kafka guarantees that the offset of message B will be higher than
        message A, and that consumers will read message B after message A.
    </li>
    <li>
        Produced messages are considered "committed" when they were written to the partition on all its in-sync replicas
        (but not necessarily flushed to disk). Producers can choose to receive acknowledgments of sent messages when the
        message was fully committed, when it was written to the leader, or when it was sent over the network.
    </li>
    <li>
        Messages that are committed will not be lost as long as at least one replica remains alive.
    </li>
    <li>
        Consumers can only read messages that are committed.
    </li>
</ul>

<p>
    These guarantees can then be "tweaked" by changing Kafka's configuration, depending on the business needs and
    trade-offs made taking into account other important considerations, such as availability, high throughput, low
    latency, and hardware costs.
</p>
<p>
    If we allow out-of-sync replicas to become leaders, we risk data loss and inconsistencies. If we don't allow them to
    become leaders, we face lower availability as we must wait for the original leader to become available before the
    partition is back online.

</p>
<p>
    There are two important things that everyone who writes applications that produce to Kafka must pay attention to:
</p>
<ul>
    <li>Use the correct <code>acks</code> configuration to match reliability requirements</li>
    <li>Handle errors correctly both in configuration and in code</li>
</ul>

<p>Some failure conditions under which integration tests can be run:</p>
<ul>
    <li>Clients lose connectivity to one of the brokers</li>
    <li>High latency between client and broker</li>
    <li>Disk full</li>
    <li>Hanging disk (also called "brown out")</li>
    <li>Leader election</li>
    <li>Rolling restart of brokers</li>
    <li>Rolling restart of consumers</li>
    <li>Rolling restart of producers</li>
</ul>

<p>
    <a href="https://github.com/apache/kafka/blob/trunk/trogdor/README.md"
       target="_blank">Trogdor</a>, the test framework for
    Apache Kafka could be used to simulate systems faults.
</p>
<p>
    <a href="https://github.com/linkedin/Burrow" target="_blank">Burrow, the Kafka Consumer Lag Checking</a> could be
    useful in helping monitoring of consumer lags.
</p>

<h2>Chapter 8: Exactly-Once Semantics</h2>

<p>Mechanisms that provide exactly-once guarantees:</p>
<ol>
    <li>idempotent producer - which avoids duplicates that are caused by the retry mechanism</li>
    <li>transactions - form the basis of exactly-once semantics in Kafka Streams</li>
</ol>

<p>
    When the idempotent producer is enabled, each message will include a unique identified producer ID (<em>PID</em>)
    and a <em>sequence number</em>, which together with the target <em>topic</em> and <em>partition</em>, uniquely
    identify each message.
</p>
<p>
    The idempotent producer will only prevent duplicates caused by the retry mechanism of the producer itself, whether
    the retry is caused by producer, network, or broker errors. But nothing else.
</p>
<p>
    Exactly-once processing means that consuming, processing, and producing are done <em>atomically</em>. Either the
    offset of the original message is committed and the result is successfully produced or neither of these things
    happen. We need to make sure that partial results—where the offset is committed but the result isn't produced, or
    vice versa&mdash;can't happen. To support this behavior, Kafka transactions introduce the idea of atomic
    multi-partition
    writes.
</p>
<p>
    A transactional producer is simply a Kafka producer that is configured with a <code>transactional.id</code> and has
    been initialized using <code>initTransactions()</code>.
</p>
<p>
    Transactions provide exactly-once guarantees when used within chains of consume-process-produce stream processing
    tasks. In other contexts, transactions will either straight-out not work or will require additional effort in order
    to achieve the guarantees we want.
</p>
<p>
    Two mistakes are assuming that exactly-once guarantees apply on actions other than producing to Kafka, and that
    consumers always read entire transactions and have information about transaction boundaries.
</p>
<p>
    The most common and most recommended way to use transactions is to enable exactly-once guarantees in Kafka Streams.
</p>
<p>
    To enable exactly-once guarantees for a Kafka Streams application, we simply set the
    <code>processing.guarantee</code> configuration to either <code>exactly_once</code> or
    <code>exactly_once_beta</code>.
</p>

<h2>Chapter 9: Building Data Pipelines</h2>

<p>
    The main value Kafka provides to data pipelines is its ability to serve as a very large, reliable buffer between
    various stages in the pipeline.
</p>

<h3>Run Kafka Connect</h3>

<pre><code class="language-shell">
bin/connect-distributed.sh config/connect-distributed.properties
</code></pre>

<p>
    The <a href="https://debezium.io/" target="_blank">Debezium Project</a> provides a collection of high-quality, open
    source, change capture connectors for a variety of databases.
</p>
<p>
    Kafka can be looked at as a platform that can handle data integration (with Connect), application integration (with
    producers and consumers), and stream processing. Kafka could be a viable replacement for an ETL tool that only
    integrates data stores.
</p>

<h2>Chapter 10: Cross-Cluster Data Mirroring</h2>

<p>Apache Kafka's built-in cross-cluster replicator is called MirrorMaker.</p>

<h3>Use Cases of Cross-Cluster Mirroring</h3>
<ul>
    <li>Regional and central clusters</li>
    <li>High availability (HA) and disaster recovery (DR)</li>
    <li>Regulatory compliance</li>
    <li>Cloud migrations</li>
    <li>Aggregation of data from edge clusters</li>
</ul>

<h3>Multicluster Architectures</h3>

<p>Some principles that should guide these architectures:</p>
<ul>
    <li>No less than one cluster per datacenter.</li>
    <li>Replicate each event exactly once (barring retries due to errors) between each pair of datacenters.</li>
    <li>Some principles that should guide these architectures:When possible, consume from a remote datacenter rather
        than produce to a remote datacenter.
    </li>
</ul>

<h4>Hub-and-Spoke Architecture</h4>

<p>
    The main benefit of this architecture is that data is always produced to the local datacenter and events from each
    datacenter are only mirrored once—to the central datacenter.
</p>
<p>
    The main drawback of this architecture is the direct result of its benefits and simplicity. Processors in one
    regional datacenter can't access data in another.
</p>

<h4>Active-Active Architecture</h4>

<p>
    One benefit of this architecture is the ability to serve users from a nearby datacenter, which typically has
    performance benefits, without sacrificing functionality due to limited availability of data.
</p>
<p>
    Another benefit is redundancy and resilience. Since every datacenter has all the functionality, if one datacenter is
    unavailable, you can direct users to a remaining datacenter. This type of failover only requires network redirects
    of users, typically the easiest and most transparent type of failover.
</p>
<p>
    The main drawback of this architecture is the challenge in avoiding conflicts when data is read and updated
    asynchronously in multiple locations.
</p>

<h4>Active-Standby Architecture</h4>

<p>
    The benefits of this setup are simplicity in setup and the fact that it can be used in pretty much any use case.
</p>
<p>
    The disadvantages are waste of a good cluster and the fact that failover between Kafka clusters is, in fact, much
    harder than it looks.
</p>

<h4>Stretch Clusters</h4>

<p>
    One advantage of this architecture is in the synchronous replication&mdash;some types of business simply require
    that their DR site is always 100% synchronized with the primary site.
</p>
<p>
    Other advantage is that both datacenters and all brokers in the cluster are used. There is no waste like in
    <em>active-standby</em> architectures.
</p>
<p>
    This architecture is limited in the type of disasters it protects against. It only protects from datacenter
    failures, not any kind of application or Kafka failures. The operational complexity is also limited. This
    architecture demands physical infrastructure that not all companies can provide.
</p>

<h3>Apache Kafka's MirrorMaker</h3>

<p>
    MirrorMaker is highly configurable. In addition to the cluster settings to define the topology, Kafka Connect, and
    connector settings, every configuration property of the underlying producer, consumers, and admin client used by
    MirrorMaker can be customized.
</p>

<p>Example: start MirrorMaker with the configuration options specified in the properties file:</p>

<pre><code class="language-shell">
bin/connect-mirror-maker.sh etc/kafka/connect-mirror-maker.properties
</code></pre>

<p>
    Any number of these processes can be started to form a dedicated MirrorMaker cluster that is scalable and
    fault-tolerant. The processes mirroring to the same cluster will find each other and balance load between them
    automatically. Usually when running MirrorMaker in a production environment, we'd want to run MirrorMaker as a
    service, running in the background with `nohup` and redirecting its console output to a log file. The tool also
    has <code>-daemon</code> as a command-line option that should do that for us.
</p>
<p>
    Production deployment systems like Ansible, Puppet, Chef, and Salt are often used to automate deployment and manage
    the many configuration options. MirrorMaker may also be run inside a Docker container. MirrorMaker is completely
    stateless and doesn't require any disk storage (all the data and state are stored in Kafka itself).
</p>

<p>When deploying MirrorMaker in production, it is important to monitor it as follows:</p>
<ul>
    <li>Kafka Connect monitoring</li>
    <li>MirrorMaker metrics monitoring</li>
    <li>Lag monitoring</li>
    <li>Producer and consumer metrics monitoring</li>
    <li>Canary</li>
</ul>

<h2>Chapter 11: Securing Kafka</h2>

<h3>
    Some security procedures applied in Kafka to establish and maintain confidentiality/integrity/availability of data
</h3>
<ul>
    <li>Authentication establishes your identity and determines _who_ you are.</li>
    <li>Authorization determines _what_ you are allowed to do.</li>
    <li>Encryption protects your data from eavesdropping and tampering.</li>
    <li>Auditing tracks what you have done or have attempted to do.</li>
    <li>Quotas control how much of the resources you can utilize.</li>
</ul>

<h3>A secure deployment must guarantee</h3>
<ul>
    <li>Client authenticity</li>
    <li>Server authenticity</li>
    <li>Data privacy</li>
    <li>Data integrity</li>
    <li>Access control</li>
    <li>Auditability</li>
    <li>Availability</li>
</ul>

<h3>Security Protocols</h3>

<p>
    Each Kafka security protocol combines a transport layer (PLAINTEXT or SSL) with an optional authentication layer
    (SSL or SASL):
</p>
<ul>
    <li><code>PLAINTEXT</code></li>
    <li><code>SSL</code></li>
    <li><code>SASL_PLAINTEXT</code></li>
    <li><code>SASL_SSL</code></li>
</ul>

<p>
    Example: Configure SSL for the inter-broker and internal listeners, and SASL_SSL for the external listener:
</p>

<pre><code class="language-properties">
listeners=EXTERNAL://:9092,INTERNAL://10.0.0.2:9093,BROKER://10.0.0.2:9094
advertised.listeners=EXTERNAL://broker1.example.com:9092,INTERNAL://broker1.local:9093,BROKER://broker1.local:9094
listener.security.protocol.map=EXTERNAL:SASL_SSL,INTERNAL:SSL,BROKER:SSL
inter.broker.listener.name=BROKER
</code></pre>

<p>
    Then clients are configured with a security protocol and bootstrap servers that determine the broker listener.
    Metadata returned to clients contains only the endpoints corresponding to the same listener as the bootstrap
    servers:
</p>
<pre><code class="language-properties">
security.protocol=SASL_SSL
bootstrap.servers=broker1.example.com:9092,broker2.example.com:9092
</code></pre>

<h4>SASL</h4>

<p>Kafka brokers support the following SASL mechanisms out of the box:</p>
<ul>
    <li><code>GSSAPI</code></li>
    <li><code>PLAIN</code></li>
    <li><code>SCRAM-SHA-256 and SCRAM-SHA-512</code></li>
    <li><code>OAUTHBEARER</code></li>
</ul>

<p>Kafka uses the Java Authentication and Authorization Service (JAAS) for configuring SASL.</p>

<h4>SASL/GSSAPI</h4>

<p>
    Generic Security Service Application Program Interface (GSS-API) is a framework for providing security services to
    applications using different authentication mechanisms.
</p>

<h4>SASL/SCRAM</h4>

<p>
    SCRAM applies a one-way cryptographic hash function on the password combined with a random salt to avoid the actual
    password being transmitted over the wire or stored in a database.
</p>
<p>
    Kafka provides safeguards by supporting only the strong hashing algorithms SHA-256 and SHA-512 and avoiding weaker
    algorithms like SHA-1. This is combined with a high default iteration count of 4,096 and unique random salts for
    every stored key to limit the impact if ZooKeeper security is compromised.
</p>

<h4>SASL/OAUTHBEARER</h4>

<p>
    Kafka supports SASL/OAUTHBEARER for client authentication, enabling integration with third-party OAuth servers. The
    built-in implementation of OAUTHBEARER uses unsecured JSON Web Tokens (JWTs) and is not suitable for production use.
    Custom callbacks can be added to integrate with standard OAuth servers to provide secure authentication using the
    OAUTHBEARER mechanism in production deployments.

</p>
<p>
    Because the built-in implementation of SASL/OAUTHBEARER in Kafka does not validate tokens, it only requires the
    login module to be specified in the JAAS configuration. If the listener is used for inter-broker communication,
    details of the token used for client connections initiated by brokers must also be provided.
</p>
<p>
    The option <code>unsecuredLoginStringClaim_sub</code> is the subject claim that determines the KafkaPrincipal for
    the connection by default. Example:
</p>
<pre><code class="language-properties">
sasl.enabled.mechanisms=OAUTHBEARER
sasl.mechanism.inter.broker.protocol=OAUTHBEARER
listener.name.external.oauthbearer.sasl.jaas.config=\
org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule \
required unsecuredLoginStringClaim_sub="kafka";
</code></pre>

<p>
    Clients must be configured with the subject claim option `unsecuredLoginStringClaim_sub`. Other claims and token
    lifetime may also be configured:
</p>
<pre><code class="language-properties">
sasl.mechanism=OAUTHBEARER
sasl.jaas.config=\
org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule \
required unsecuredLoginStringClaim_sub="John";
</code></pre>

<p>
    To integrate Kafka with third-party OAuth servers for using bearer tokens in production, Kafka clients must be
    configured with <code>sasl.login.callback.handler.class</code> to acquire tokens from the OAuth server using the
    long-term password or a refresh token.
</p>
<p>
    If OAUTHBEARER is used for inter-broker communication, brokers must also be configured with a login callback
    handler to acquire tokens for client connections created by the broker for inter-broker communication.
</p>
<p>
    To create and validate delegation tokens, all brokers must be configured with the same master key using the
    configuration option <code>delegation.token.master.key</code>.
</p>

<p>
    Delegation tokens also are suitable for production use only in deployments where ZooKeeper is secure.
</p>

<h3>Reauthentication</h3>

<p>
    Kafka brokers support reauthentication for connections authenticated using SASL using the configuration option
    <code>connections.max.reauth.ms</code>.
</p>

<h3>Authorization</h3>

<p>Kafka brokers manage access control using a customizable authorizer.</p>
<p>
    Kafka has a built-in authorizer, `AclAuthorizer`, that can be enabled by configuring the authorizer class name as
    follows:
</p>
<pre><code class="language-properties">
authorizer.class.name=kafka.security.authorizer.AclAuthorizer
</code></pre>

<p>
    Super users are granted access for all operations on all resources without any restrictions and cannot be denied
    access using Deny ACLs.
</p>

<h3>Security Considerations</h3>

<p>
    Since AclAuthorizer stores ACLs in ZooKeeper, access to ZooKeeper should be restricted. Deployments without a secure
    ZooKeeper can implement custom authorizers to store ACLs in a secure external database.
</p>
<p>
    Restricting user access using the principle of least privilege can limit exposure if a user is compromised.
</p>
<p>
    ACLs should be removed immediately when a user principal is no longer in use, for instance, when a person leaves the
    organization.
</p>
<p>
    Long-running applications can be configured with service credentials rather than credentials associated with a
    specific user to avoid any disruption when employees leave the organization.
</p>

<h3>Auditing</h3>

<p>
    Kafka brokers can be configured to generate comprehensive _log4j_ logs for auditing and debugging. The logging level
    as well as the appenders used for logging and their configuration options can be specified in
    <code>log4j.properties</code>.
</p>

<h3>Securing the platform</h3>

<p>Kafka supports externalizing passwords in a secure store.</p>
<p>
    Customizable configuration providers can be configured for Kafka brokers and clients to retrieve
    passwords from a secure third-party password store.
</p>
<p>
    Passwords may also be stored in encrypted form in configuration files with custom configuration
    providers that perform decryption.

</p>
<p>
    Sensitive broker configuration options can also be stored encrypted in ZooKeeper using the Kafka
    configs tool without using custom providers.
</p>

<h2>Chapter 12: Administering Kafka</h2>

<p>
    While Apache Kafka implements authentication and authorization to control topic operations, default configurations
    do not restrict the use of these tools. This means that these CLI tools can be used without any authentication
    required, which will allow operations such as topic changes to be executed with no security check or audit.
</p>

<h3>Topic Operations</h3>

<p>
    The <code>kafka-topics.sh</code> tool provides access to most topic operations. It allows us to create,
    modify, delete, and list information about topics in the cluster. While some topic configurations are possible
    through this command, they have been deprecated, and it is recommended to use the more robust method of using the
    <code>kafka-config.sh</code> tool for configuration changes.
</p>

<h3>Consumer Groups</h3>

<p>
    The <code>kafka-consumer-groups.sh</code> tool helps manage and gain insight into the consumer groups that are
    consuming from topics in the cluster.
</p>

<h3>Dynamic Configuration Changes</h3>

<p>
    The <code>kafka-configs.sh</code> is the main tool for modifying all the configurations for topics, clients,
    brokers, and more that can be updated dynamically during runtime without having to shut down or redeploy a cluster.
</p>

<h3>Producing and Consuming</h3>

<p>
    To manually produce or consume some sample messages these tools are provided: <code>kafka-console-consumer.sh</code>
    and <code>kafka-console-producer.sh</code>.
</p>

<h4>Example</h4>

<p>Consuming the earliest message from the <code>__consumer_offsets</code> topic:</p>
<pre><code class="language-shell">
kafka-console-consumer.sh --bootstrap-server localhost:9092
                          --topic __consumer_offsets --from-beginning --max-messages 1
                          --formatter "kafka.coordinator.group.GroupMetadataManager\$OffsetsMessageFormatter"
                          --consumer-property exclude.internal.topics=false
</code></pre>

<p>This will show something like this:</p>
<pre><code class="language-shell">
[my-group-name,my-topic,0]::[OffsetMetadata[1,NO_METADATA]
CommitTime 1623034799990 ExpirationTime 1623639599990]
Processed a total of 1 messages
</code></pre>

<h3>Partition Management</h3>

<p>
    A default Kafka installation contains a few scripts for working with the management of partitions. One of these
    tools allows for the reelection of leader replicas; another is a low-level utility for assigning partitions to
    brokers. Together these tools can assist in situations where a more manual hands-on approach to balance message
    traffic within a cluster of Kafka brokers is needed.
</p>

<p>The <code>kafka-leader-election.sh</code> utility can be used to trigger manually a new leader election.</p>

<h4>Example</h4>

<p>Start a preferred leader election for all topics in a cluster:</p>
<pre><code class="language-shell">
kafka-leader-election.sh --bootstrap-server localhost:9092 --election-type PREFERRED --all-topic-partitions
</code></pre>

<p>
    It is also possible to start elections on specific partitions or topics. This can be done by passing in a topic name
    with the `--topic` option and a partition with the `--partition` option directly. It is also possible to pass in a
    list of several partitions to be elected through a JSON file with the topic names.
</p>

<h4>Example</h4>

<p>
    Start a preferred replica election with a specified list of partitions in a file named <code>partitions.json</code>,
    with the following content:
</p>

<pre><code class="language-json">
{
    "partitions": [
        {
            "partition": 1,
            "topic": "my-topic"
        },
        {
            "partition": 2,
            "topic": "foo"
        }
    ]
}
</code></pre>

<pre><code class="language-shell">
kafka-leader-election.sh --bootstrap-server localhost:9092 --election-type PREFERRED --path-to-json-file partitions.json
</code></pre>

<p>
    The <code>kafka-reassign-partitions.sh</code> can be used to change the replica assignments manually for a
    partition.
</p>

<p>The <code>kafka-dump-log.sh</code> tool can be used to decode the log segments for a partition.</p>

<p>
    The <code>kafka-replica-verification.sh</code> tool can be used to validate that the replicas for a topic's
    partitions are the same across the cluster.
</p>

<h3>Unsafe Operations</h3>
<ul>
    <li>Moving the Cluster Controller</li>
    <li>Removing Topics to Be Deleted</li>
    <li>Deleting Topics Manually</li>
</ul>

<h2>Chapter 13: Monitoring Kafka</h2>

<p>
    If the metrics are consumed by automation, they should be very specific. It's OK to have a large number of metrics,
    each describing small details, because computers process a lot of data easily. The more specific the data is,
    the easier it is to create automation that acts on it, because the data does not leave as much room for
    interpretation as to its meaning. On the other hand, if the metrics will be consumed by humans, presenting a large
    number of metrics will be overwhelming. This becomes even more important when defining alerts based on those
    measurements.
</p>

<h3>Application Health Checks</h3>

<p>Two ways to do a health check:</p>
<ul>
    <li>An external process that reports whether the broker is up or down (health check)</li>
    <li>Alerting on the lack of metrics being reported by the Kafka broker (sometimes called stale metrics)</li>
</ul>

<h3>Service-Level Objectives (SLOs)</h3>

<p>
    A <em>service-level indicator</em> (SLI) is a metric that describes one aspect of a service's reliability.
</p>
<p>
    A <em>service-level objective</em> (SLO), which can also be called a <em>service-level threshold</em> (SLT),
    combines an SLI with a target value.
</p>

<blockquote>
    The term <em>operational-level agreement</em> (OLA) is less frequently used. It describes agreements between
    multiple internal services or support providers in the overall delivery of an SLA.
</blockquote>

<p>Further reads to dive deeper into these topics are:</p>
<ul>
    <li>
        <a href="https://www.oreilly.com/library/view/site-reliability-engineering/9781491929117/" target="_blank">
            Site Reliability Engineering
        </a>
    </li>
    <li>
        <a href="https://www.oreilly.com/library/view/the-site-reliability/9781492029496/" target="_blank">
            The Site Reliability Workbook
        </a>
    </li>
</ul>

<h3>Kafka Broker Metrics</h3>

<h4>Diagnosing Cluster Problems</h4>

<p>Categories of cluster problems:</p>
<ul>
    <li>Single-broker problems</li>
    <li>Overloaded clusters</li>
    <li>Controller problems</li>
</ul>

<p>
    The under-replicated partitions measurement, provided on each broker in a cluster, gives a count of the number of
    partitions for which the broker is the leader replica, where the follower replicas are not caught up. This
    measurement provides insight into a number of problems with the Kafka cluster, from a broker being down to resource
    exhaustion.
</p>

<p>List under-replicated partitions in a cluster:</p>
<pre><code class="language-shell">
kafka-topics.sh --bootstrap-server kafka1.example.com:9092/kafka-cluster --describe --under-replicated
</code></pre>

<p>
    Cluster-level problems are usually either <em>Unbalanced load</em> or <em>resource exhaustion</em>.
</p>
<p>Host level problems can be any of:</p>
<ul>
    <li>Hardware failures</li>
    <li>Networking</li>
    <li>Conflicts with another process</li>
    <li>Local configuration differences</li>
</ul>

<p>
    Configuration management system, such as <a href="https://www.chef.io/" target="_blank">Chef</a> or
    <a href="https://www.puppet.com/" target="_blank">Puppet</a> can be used to maintain consistent configurations
    across the operating systems running the Kafka brokers.
</p>
<p>
    In general, the number of request handler threads should be set equal to the number of processors in the system,
    including hyperthreaded processors.
</p>

<h4>Logging</h4>

<p>
    Two loggers write to separate files on disk: <code>kafka.controller</code> and
    <code>kafka.server.ClientQuotaManager</code>, both at the <code>INFO</code> level.
</p>
<p>
    The <code>kafka.request.logger</code> is also useful when debugging issues with Kafka, turned on at either
    <code>DEBUG</code> or <code>TRACE</code> level.
</p>

<h3>Client Monitoring</h3>

<h4>Producer Metrics</h4>

<p>
    The <strong>overall producer metrics</strong> bean provides attributes describing everything from the
    sizes of the message batches to the memory buffer utilization.
</p>

<h4>Consumer Metrics</h4>

<p>
    The overall consumer bean has metrics regarding the lower-level network operations, and the fetch manager bean has
    metrics regarding bytes, request, and record rates.
</p>
<p>
    Unlike the producer client, the metrics provided by the consumer are useful to look at but not useful
    for setting up alerts on.
</p>

<h3>Quotas</h3>

<p>
    A Kafka broker does not use error codes in the response to indicate a client is being throttled. So, it's not
    obvious to and application that throttling is happening without monitoring the metrics that are provided to show
    the amount of time that the client is being throttled. The metrics that must be monitored are:
</p>
<ul>
    <li>(Consumer) bean
        <code>
            kafka.consumer:type=consumer-fetch-manager-metrics,client-id=&lt;THE_CLIENTID&gt;,attribute
            fetch-throttle-time-avg
        </code>
    </li>
    <li>(Producer) bean
        <code>
            kafka.producer:type=producer-metrics,client-id=&lt;THE_CLIENTID&gt;,attribute produce-throttle-time-avg
        </code>
    </li>
</ul>

<h3>Lag Monitoring</h3>

<p>
    For Kafka consumers, the most important thing to monitor is the consumer lag (the difference between the last
    message produced in a specific partition and the last message processed by the consumer).
</p>
<p>
    The preferred method of consumer lag monitoring is to have an external process that
    can watch both the state of the partition on the broker, tracking the offset of the most
    recently produced message, and the state of the consumer, tracking the last offset the
    consumer group has committed for the partition.
</p>
<p>
    <a href="https://github.com/linkedin/Burrow" target="_blank">Burrow</a> can be used to monitor consumer groups.
    More on this can be checked at
    <a href="https://engineering.linkedin.com/apache-kafka/burrow-kafka-consumer-monitoring-reinvented" target="_blank">
        this blog post
    </a>.
</p>
<p>
    <a href="https://github.com/linkedin/kafka-monitor" target="_blank">Xinfra Monitor</a> can be used to continually
    produce and consume data from a topic that is spread across all brokers in a cluster. This way, we can monitor
    if we can produce messages to the broker(s) and if we can consume them. It measures the availability of both produce
    and consume requests on each broker, as well as the total produce to consume latency.
</p>

<h2>Chapter 14: Stream Processing</h2>

<p>
    Starting from version 0.10.0, Kafka includes a stream processing library as part of its collection of client
    libraries, called Kafka Streams (Streams API). It allows developers to consume, process, and produce events in their
    apps, without relying on an external processing framework.
</p>

<h3>What Is Stream Processing?</h3>

<p>
    Data stream (event stream, streaming data) is an abstraction that represents an unbounded data. Here,
    <em>unbounded</em> means infinite and ever-growing. That's because the dataset is unbounded, as, over time, new
    records keep arriving. Some of its characteristics are:
</p>
<ul>
    <li>Event streams are ordered</li>
    <li>Immutable data records</li>
    <li>Event streams are replayable</li>
</ul>

<p>
    Stream processing refers to the continuous processing of event streams. It's a programming paradigm, just like
    request-response and batch processing are.
</p>
<blockquote>
    In a databases context, online transaction processing (OLTP) is the equivalente to request-response is.
</blockquote>

<p>
    The collection of all the information that can be handled by the stream processing pipeline, such as counting the
    number of events by type, moving averages, and joining two streams to create an enriched stream of information is
    called a <em>state</em>. In this case it's not enough to look at each event by itself; we need to keep track of
    more information &mdash; how many events of each type did we see this hour, all events that require joining,
    sums, averages, etc.
</p>
<p>
    Kafka Streams uses Kafka’s transactions to implement exactly-once guarantees for stream processing applications.
    The <em>exactly-once guarantees</em> provided by the Kafka Streams library can be enabled by setting
    <code>processing.guarantee</code> to <code>exactly_once</code>.
</p>

<h3>Stream Processing Design Patterns</h3>

<ul>
    <li>
        <em>Single-Event Processing:</em> Processing of each event in isolation &mdash; the most basic pattern.
    </li>
    <li>
        <em>Processing with Local State:</em> Used when the information being calculated/aggregated requires a
        <em>local state</em> (rather than a shared state). Some of the issues that must be
        addressed when using this pattern are:
        <ul>
            <li>Memory usage</li>
            <li>Persistence</li>
            <li>Rebalancing</li>
        </ul>
    </li>
    <li>
        <em>Multiphase Processing/Repartitioning:</em> A "two-phase approach". There is a first step where some
        calculation happens and then the results are written to a new topic with a single partition. This partition will
        be read by a single application instance that can then find the previously calculated results. The calculation
        can be performed on each instance with a local state. Sometimes more steps are needed to produce the final
        results.
    </li>
    <li>
        <em>Processing with External Lookup: Stream-Table Join:</em> One of the streams represents changes
        to a locally cached table. It's used when we want to enrich the information from the information stream with
        information stored in an external storage system, like an external database. A common approach to keep the
        local cache up-to-date is to perform some Change Data Capture (CDC) from the external database to keep the
        local cache updated.
    </li>
    <li>
        <em>Table-Table Join:</em> Represents a join between two tables at the time the operation is performed. With
        Kafka Streams, we can perform <code>equi-join</code> and <code>foreign-key join</code>.
    </li>
    <li>
        <em>Streaming Join:</em> Matches events from one stream with events from another stream that have the same key
        and happened in the same time window.
    </li>
    <li>
        <em>Out-of-Sequence Events:</em> Handles events that arrive at an "incorrect" time. Examples of such events are
        mobiles that send events after a network recovery or when monitoring faulty network equipment.
    </li>
    <li>
        <em>Reprocessing</em>, in its two variants:
        <ul>
            <li>
                An improved version of the processing app is run against the same (old) stream, the new processing
                result is compared to the initial one (not replaced), and then move clients to use the new result.
            </li>
            <li>
                The old stream is processed by an updated application and the results are recalculated.
            </li>
        </ul>
    </li>
    <li>
        <em>Interactive Queries:</em> Read the results from the processing application state store itself, as apposed to
        reading the results from an output topic.
    </li>
</ul>

<h3>The Stream Processing Library</h3>

<p>
    Apache Kafka has two stream APIs: a low-level
    <a href="https://kafka.apache.org/28/documentation/streams/developer-guide/processor-api.html" target="_blank">
        Processor API
    </a>
    and a high-level Streams DSL (see
    <a href="https://events19.linuxfoundation.org/wp-content/uploads/2017/12/Beyond-the-DSL%E2%80%94Unlocking-the-Power-of-Kafka-Streams-with-the-Processor-API-Antony-Stubbs-Confluent-Inc..pdf"
       target="_blank">
        presentation
    </a>).
</p>
<p>
    Some examples:
</p>
<ul>
    <li>
        <a href="https://github.com/gwenshap/kafka-streams-wordcount" target="_blank">Word Count</a>
    </li>
    <li>
        <a href="https://github.com/gwenshap/kafka-streams-stockstats" target="_blank">Stock Market Statistics</a>
    </li>
    <li>
        <a href="https://github.com/gwenshap/kafka-clickstream-enrich" target="_blank">ClickStream Enrichment</a>
    </li>
</ul>

<h3>Kafka Streams: Architecture Overview</h3>

<p>
    Every streams application implements and executes one topology (or <em>DAG</em> &mdash; or directed acyclic graph).
</p>
<p>The main testing tool for Kafka Streams applications is <code>TopologyTestDriver</code>.</p>
<p>
    Two popular integration test frameworks used for testing Kafka application are: <em>EmbeddedKafkaCluster</em> and
    <em>Testcontainers</em>.
</p>
<p>
    The blog post
    <a href="https://www.confluent.io/blog/testing-kafka-streams/" target="_blank">
        Testing Kafka Streams &mdash; A Deep Dive
    </a> has deep explanations and detailed code examples of topologies and tests.
</p>
<p>
    Kafka Streams handles scenarios where there are dependencies between tasks (i.e.: because a processing step
    requires an input from multiple partitions) by assigning all the partitions needed for one join to the same task so
    the task can consume from all the relevant partitions and perform the join independently.
</p>
